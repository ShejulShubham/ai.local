# ai.local

**ai.local** is a local AI chatbot system using [Ollama](https://ollama.com/) as the backend and a web-based frontend served through Nginx.

This project includes **two architecture designs** for different environments:

- ðŸ§ Linux system
- ðŸªŸ Windows system (with WSL for containers)

Each setup keeps Ollama installed on the host, and runs the frontend via containers.

---

## ðŸ§ Linux System Architecture (./Linux)

In this configuration:

- **Ollama is installed directly on the Linux host**
- **Frontend (Open WebUI)** and **Nginx (reverse proxy)** are both containerized

### ðŸ”„ Flow Diagram (Linux)

```text
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚    Client Browser (User)   â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚    NGINX       â”‚ (Docker)
                       â”‚  Reverse Proxy â”‚
                       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â–¼                              â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚  Frontend    â”‚   HTTP API  â”‚     Ollama       â”‚
      â”‚ (Open WebUI) â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  (Host Installed)â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
````

---

## ðŸªŸ Windows System Architecture (./Windows)

In this setup:

* **Ollama is installed natively on Windows**
* **Docker (running inside WSL)** hosts the frontend and Nginx containers
* Communication occurs between WSL containers and the Windows host Ollama backend via host networking

### ðŸ”„ Flow Diagram (Windows + WSL)

```text
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚    Client Browser (User)   â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚    NGINX       â”‚ (Docker in WSL)
                       â”‚  Reverse Proxy â”‚
                       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â–¼                              â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚  Frontend    â”‚   HTTP API  â”‚     Ollama       â”‚
      â”‚ (Open WebUI) â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ (Windows Native) â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> ðŸ“Œ Make sure the WSL containers can reach Ollama on the Windows host IP (e.g., `host.docker.internal` or local network IP).

---

## ðŸ’¡ Summary

| Component     | Linux                   | Windows (with WSL)           |
| ------------- | ----------------------- | ---------------------------- |
| Ollama        | Host-installed (native) | Windows-native               |
| Frontend      | Docker container        | Docker in WSL                |
| NGINX         | Docker container        | Docker in WSL                |
| Communication | Localhost or bridge     | `host.docker.internal` or IP |

---